---
# Source: ingress-nginx/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: kube-system
automountServiceAccountToken: true
---
# Source: ingress-nginx/templates/controller-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: kube-system
data:
  allow-snippet-annotations: "true"
---
# Source: ingress-nginx/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
  name: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
      - namespaces
    verbs:
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
      - get
---
# Source: ingress-nginx/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: "kube-system"
---
# Source: ingress-nginx/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: kube-system
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    resourceNames:
      - ingress-nginx-leader
    verbs:
      - get
      - update
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
      - get
---
# Source: ingress-nginx/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: "kube-system"
---
# Source: ingress-nginx/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: kube-system
spec:
  type: LoadBalancer
  ipFamilyPolicy: SingleStack
  ipFamilies: 
    - IPv4
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
      appProtocol: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
      appProtocol: https
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller
---
# Source: ingress-nginx/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/component: controller
  replicas: 1
  revisionHistoryLimit: 10
  minReadySeconds: 0
  template:
    metadata:
      labels:
        helm.sh/chart: ingress-nginx-4.7.2
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/version: "1.8.2"
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: controller
    spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: controller
          image: "registry.k8s.io/ingress-nginx/controller:v1.8.2@sha256:74834d3d25b336b62cabeb8bf7f1d788706e2cf1cfd64022de4137ade8881ff2"
          imagePullPolicy: IfNotPresent
          lifecycle: 
            preStop:
              exec:
                command:
                - /wait-shutdown
          args:
            - /nginx-ingress-controller
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
            - --election-id=ingress-nginx-leader
            - --controller-class=k8s.io/ingress-nginx
            - --ingress-class=nginx
            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
            - --watch-ingress-without-class=true
          securityContext: 
            capabilities:
              drop:
              - ALL
              add:
              - NET_BIND_SERVICE
            runAsUser: 101
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: LD_PRELOAD
              value: /usr/local/lib/libmimalloc.so
          livenessProbe: 
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe: 
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          resources: 
            requests:
              cpu: 100m
              memory: 90Mi
      nodeSelector: 
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
---
# Source: ingress-nginx/templates/controller-ingressclass.yaml
# We don't support namespaced ingressClass yet
# So a ClusterRole and a ClusterRoleBinding is required
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.7.2
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: "1.8.2"
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: nginx
spec:
  controller: k8s.io/ingress-nginx

---
# Source: http-debug/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: http-debug
  labels:
    helm.sh/chart: http-debug-0.1.0
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: http-debug/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: http-debug
  labels:
    helm.sh/chart: http-debug-0.1.0
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
---
# Source: http-debug/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: http-debug
  labels:
    helm.sh/chart: http-debug-0.1.0
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 5
  selector:
    matchLabels:
      app.kubernetes.io/name: http-debug
      app.kubernetes.io/instance: http-debug
  template:
    metadata:
      labels:
        app.kubernetes.io/name: http-debug
        app.kubernetes.io/instance: http-debug
    spec:
      serviceAccountName: http-debug
      securityContext:
        {}
      containers:
        - name: http-debug
          securityContext:
            {}
          image: "bicarus/http-https-echo:27"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8888
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
          env:
            - name: HTTP_PORT
              value: "8888"
            - name: HTTPS_PORT
              value: "9999"
---
# Source: http-debug/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: http-debug
  labels:
    helm.sh/chart: http-debug-0.1.0
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    nginx.ingress.kubernetes.io/auth-response-headers: Set-Cookie,X-authentik-username,X-authentik-groups,X-authentik-email,X-authentik-name,X-authentik-uid
    nginx.ingress.kubernetes.io/auth-signin: https://debug.127-0-0-1.nip.io/outpost.goauthentik.io/start?rd=$escaped_request_uri
    nginx.ingress.kubernetes.io/auth-snippet: |
      proxy_set_header X-Forwarded-Host $http_host;
    nginx.ingress.kubernetes.io/auth-url: http://ak-outpost-default.authentik.svc.cluster.local:9000/outpost.goauthentik.io/auth/nginx
spec:
  rules:
    - host: "debug.127-0-0-1.nip.io"
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: http-debug
                port:
                  number: 80
---
# Source: http-debug/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "http-debug-test-connection"
  labels:
    helm.sh/chart: http-debug-0.1.0
    app.kubernetes.io/name: http-debug
    app.kubernetes.io/instance: http-debug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['http-debug:80']
  restartPolicy: Never

---
# Source: authentik/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: authentik-redis
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
---
# Source: authentik/charts/serviceAccount/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: authentik
  namespace: authentik
  labels:
    helm.sh/chart: serviceAccount-1.2.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: authentik/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: authentik-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.16.2
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
  namespace: authentik
type: Opaque
data:
  postgresql-postgres-password: "OFRFMkN5eEVpaQ=="
  postgresql-password: "YQ=="
---
# Source: authentik/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: authentik
  labels:
    helm.sh/chart: authentik-2023.8.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.8.2"
    app.kubernetes.io/managed-by: Helm
data:  
  AUTHENTIK_EMAIL__PORT: "NTg3"
  AUTHENTIK_EMAIL__TIMEOUT: "MzA="
  AUTHENTIK_EMAIL__USE_SSL: "ZmFsc2U="
  AUTHENTIK_EMAIL__USE_TLS: "ZmFsc2U="
  AUTHENTIK_ERROR_REPORTING__ENABLED: "dHJ1ZQ=="
  AUTHENTIK_ERROR_REPORTING__ENVIRONMENT: "azhz"
  AUTHENTIK_ERROR_REPORTING__SEND_PII: "ZmFsc2U="
  AUTHENTIK_GEOIP: "L2dlb2lwL0dlb0xpdGUyLUNpdHkubW1kYg=="
  AUTHENTIK_LOG_LEVEL: "ZGVidWc="
  AUTHENTIK_OUTPOSTS__CONTAINER_IMAGE_BASE: "Z2hjci5pby9nb2F1dGhlbnRpay8lKHR5cGUpczolKHZlcnNpb24pcw=="
  AUTHENTIK_POSTGRESQL__HOST: "YXV0aGVudGlrLXBvc3RncmVzcWw="
  AUTHENTIK_POSTGRESQL__NAME: "YXV0aGVudGlr"
  AUTHENTIK_POSTGRESQL__PASSWORD: "YQ=="
  AUTHENTIK_POSTGRESQL__PORT: "NTQzMg=="
  AUTHENTIK_POSTGRESQL__USER: "YXV0aGVudGlr"
  AUTHENTIK_REDIS__HOST: "YXV0aGVudGlrLXJlZGlzLW1hc3Rlcg=="
  AUTHENTIK_SECRET_KEY: "YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWE="
---
# Source: authentik/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: authentik-redis-configuration
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    slave-read-only yes
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: authentik/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: authentik-redis-health
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: authentik/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: authentik-redis-scripts
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: authentik/charts/serviceAccount/templates/cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: authentik-authentik
  labels:
    helm.sh/chart: serviceAccount-1.2.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.6.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
---
# Source: authentik/charts/serviceAccount/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: authentik-authentik
  labels:
    helm.sh/chart: serviceAccount-1.2.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: authentik-authentik
subjects:
  - kind: ServiceAccount
    name: authentik
    namespace: authentik
---
# Source: authentik/charts/serviceAccount/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: authentik
  namespace: authentik
  labels:
    helm.sh/chart: serviceAccount-1.2.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.6.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
      - services
      - configmaps
    verbs:
      - get
      - create
      - delete
      - list
      - patch
  - apiGroups:
      - extensions
      - apps
    resources:
      - deployments
    verbs:
      - get
      - create
      - delete
      - list
      - patch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - create
      - delete
      - list
      - patch
  - apiGroups:
      - traefik.containo.us
      - traefik.io
    resources:
      - middlewares
    verbs:
      - get
      - create
      - delete
      - list
      - patch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
    verbs:
      - get
      - create
      - delete
      - list
      - patch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - list
---
# Source: authentik/charts/serviceAccount/templates/role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: authentik
  namespace: authentik
  labels:
    helm.sh/chart: serviceAccount-1.2.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: authentik
subjects:
  - kind: ServiceAccount
    name: authentik
    namespace: authentik
---
# Source: authentik/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentik-postgresql-headless
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.16.2
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  namespace: authentik
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: authentik
---
# Source: authentik/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentik-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.16.2
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
  annotations:
  namespace: authentik
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: authentik
    role: primary
---
# Source: authentik/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentik-redis-headless
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: authentik
---
# Source: authentik/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentik-redis-master
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/component: master
---
# Source: authentik/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentik
  labels:
    helm.sh/chart: authentik-2023.8.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.8.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9300
      name: http-metrics
      protocol: TCP
      targetPort: http-metrics
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/component: "server"
---
# Source: authentik/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: authentik-server
  labels:
    helm.sh/chart: authentik-2023.8.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.8.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "server"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: authentik
      app.kubernetes.io/instance: authentik
      app.kubernetes.io/component: "server"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: authentik
        app.kubernetes.io/instance: authentik
        app.kubernetes.io/component: "server"
        app.kubernetes.io/version: "2023.8.2"
      annotations:
        goauthentik.io/config-checksum: 5fd53094dbbaee8e355813761a9fef6feaf7c5d78dbf04e3e6abfe46dab92ca6
    spec:
      enableServiceLinks: true
      securityContext:
        {}
      containers:
        - name: authentik
          image: "ghcr.io/goauthentik/server:2023.8.2"
          imagePullPolicy: "IfNotPresent"
          args: ["server"]
          env:
            - name: "AUTHENTIK_BOOTSTRAP_PASSWORD"
              value: "akadmin"
            - name: "AUTHENTIK_BOOTSTRAP_TOKEN"
              value: "aktoken"
          envFrom:
            - secretRef:
                name: authentik
          volumeMounts:
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: http-metrics
              containerPort: 9300
              protocol: TCP
            - name: https
              containerPort: 9443
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/health/live/
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/health/live/
              port: http
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /-/health/ready/
              port: http
            periodSeconds: 10
          securityContext:
            {}
      volumes:
---
# Source: authentik/templates/worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: authentik-worker
  labels:
    helm.sh/chart: authentik-2023.8.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.8.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "worker"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: authentik
      app.kubernetes.io/instance: authentik
      app.kubernetes.io/component: "worker"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: authentik
        app.kubernetes.io/instance: authentik
        app.kubernetes.io/component: "worker"
        app.kubernetes.io/version: "2023.8.2"
      annotations:
        goauthentik.io/config-checksum: 5fd53094dbbaee8e355813761a9fef6feaf7c5d78dbf04e3e6abfe46dab92ca6
    spec:
      serviceAccountName: authentik
      enableServiceLinks: true
      securityContext:
        {}
      containers:
        - name: authentik
          image: "ghcr.io/goauthentik/server:2023.8.2"
          imagePullPolicy: "IfNotPresent"
          args: ["worker"]
          env:
            - name: "AUTHENTIK_BOOTSTRAP_PASSWORD"
              value: "akadmin"
            - name: "AUTHENTIK_BOOTSTRAP_TOKEN"
              value: "aktoken"
          envFrom:
            - secretRef:
                name: authentik
          volumeMounts:
          securityContext:
            {}
      volumes:
---
# Source: authentik/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: authentik-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.16.2
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
  namespace: authentik
spec:
  serviceName: authentik-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: authentik
      role: primary
  template:
    metadata:
      name: authentik-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-10.16.2
        app.kubernetes.io/instance: authentik
        app.kubernetes.io/managed-by: Helm
        role: primary
        app.kubernetes.io/component: primary
    spec:      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: authentik
                    app.kubernetes.io/component: primary
                namespaces:
                  - "authentik"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      automountServiceAccountToken: false
      containers:
        - name: authentik-postgresql
          image: docker.io/bitnami/postgresql:15.4.0-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: authentik-postgresql
                  key: postgresql-postgres-password
            - name: POSTGRES_USER
              value: "authentik"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: authentik-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "authentik"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: "500"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "authentik" -d "dbname=authentik" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "authentik" -d "dbname=authentik" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: authentik/charts/redis/templates/master/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: authentik-redis-master
  namespace: "authentik"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-15.7.6
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: authentik
      app.kubernetes.io/component: master
  serviceName: authentik-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-15.7.6
        app.kubernetes.io/instance: authentik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 27644827c04a13bace860bf20465b6f922d49f69b7fc857ec4af175779bdc9f1
        checksum/health: c9fa10bf90885035fbbb0f3db9ec0a7a819eac5fd6d24bb8bfb6964afcc25d60
        checksum/scripts: a7577573d6e451c289421a42bbc486c47c3b0aff44ebaedd10a381e63c9a310e
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: authentik-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: authentik
                    app.kubernetes.io/component: master
                namespaces:
                  - "authentik"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.2.10-debian-11-r13
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: authentik-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: authentik-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: authentik-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: authentik
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: authentik/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: authentik
  labels:
    helm.sh/chart: authentik-2023.8.2
    app.kubernetes.io/name: authentik
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/version: "2023.8.2"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "authentik.127-0-0-1.nip.io"
      http:
        paths:
          - path: "/"
            pathType: Prefix
            backend:
              service:
                name: authentik
                port:
                  name: http

